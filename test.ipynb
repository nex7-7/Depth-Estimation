{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "mat_file = h5py.File('nyu_depth_v2_labeled.mat', 'r')\n",
    "\n",
    "# Extract RGB images and depth maps\n",
    "rgb_images = np.array(mat_file['images'])  # Expected shape: (N, 3, H, W)\n",
    "depth_maps = np.array(mat_file['depths'])   # Expected shape: (N, H, W)\n",
    "\n",
    "# Normalize RGB images (from [0, 255] to [0, 1])\n",
    "rgb_images = rgb_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Normalize depth maps (scale between 0 and 1)\n",
    "depth_maps = depth_maps.astype(np.float32)\n",
    "depth_maps /= np.max(depth_maps)  # Normalize depth values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# Note: We add the channel dimension here so each depth map becomes (1, H, W)\n",
    "rgb_tensors = torch.tensor(rgb_images)\n",
    "depth_tensors = torch.tensor(depth_maps).unsqueeze(1)  # Shape: (N, 1, H, W)\n",
    "\n",
    "class NYUDepthDataset(Dataset):\n",
    "    def __init__(self, rgb_data, depth_data):\n",
    "        self.rgb_data = rgb_data\n",
    "        self.depth_data = depth_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each item: (RGB image with shape (3, H, W), Depth map with shape (1, H, W))\n",
    "        return self.rgb_data[idx], self.depth_data[idx]\n",
    "\n",
    "# Create dataset and DataLoader (using a batch size of 16)\n",
    "dataset = NYUDepthDataset(rgb_tensors, depth_tensors)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OptimizedDepthCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimizedDepthCNN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Increased filters\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # Learned upsampling\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Tanh()  # Better for depth normalization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Enable model compilation for speedup (PyTorch 2.x)\n",
    "model = OptimizedDepthCNN()\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.SmoothL1Loss(beta=0.1)  # Huber Loss is better for depth estimation\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)  # AdamW with decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import re\n",
    "\n",
    "num_epochs = 1\n",
    "latest_epoch = 0\n",
    "num_workers = max(0, os.cpu_count() // 2)  # Dynamic worker selection\n",
    "\n",
    "# Snapshot Directory\n",
    "snapshot_dir = \"model_snapshots\"\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "\n",
    "# Model Path\n",
    "model_pattern = re.compile(r\"depth_model_epoch_(\\d+)\\.pth\")\n",
    "\n",
    "# List all files in the directory and find matching model checkpoints\n",
    "model_files = [f for f in os.listdir(snapshot_dir) if model_pattern.match(f)]\n",
    "\n",
    "if model_files:\n",
    "    # Extract epoch numbers and find the latest one\n",
    "    latest_model = max(model_files, key=lambda f: int(model_pattern.match(f).group(1)))\n",
    "    latest_epoch = int(model_pattern.match(latest_model).group(1))\n",
    "    model_path = os.path.join(snapshot_dir, latest_model)\n",
    "\n",
    "    # Load the model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Loaded model from: {model_path} (Epoch {latest_epoch})\")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for rgb_batch, depth_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(rgb_batch)\n",
    "        depth_batch_resized = torch.nn.functional.interpolate(\n",
    "            depth_batch, size=(output.shape[2], output.shape[3]), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        \n",
    "        loss = loss_function(output, depth_batch_resized)  \n",
    "            \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Save model after each epoch\n",
    "    torch.save(model.state_dict(), f\"{snapshot_dir}/depth_model_epoch_{latest_epoch + epoch}.pth\")\n",
    "    print(f\"Model saved: {snapshot_dir}/depth_model_epoch_{latest_epoch + epoch}.pth\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a test RGB image\n",
    "test_rgb = cv2.imread('test_image_2.jpg')\n",
    "show_img = test_rgb.copy()\n",
    "test_rgb = cv2.resize(test_rgb, (640, 480)) / 255.0  # Resize and normalize\n",
    "test_rgb_tensor = torch.tensor(test_rgb).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "# Predict depth\n",
    "snapshot_dir = \"model_snapshots\"\n",
    "model_path = f\"{snapshot_dir}/depth_model_epoch_1\"\n",
    "model = OptimizedDepthCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predicted_depth = model(test_rgb_tensor).squeeze().numpy()\n",
    "\n",
    "# Display predicted depth map\n",
    "plt.subplot(1, 2, 1).imshow(cv2.cvtColor(show_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Test Image\")\n",
    "plt.subplot(1, 2, 2).imshow(predicted_depth, cmap='jet')\n",
    "plt.title(\"Predicted Depth Map\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
